
\chapter{Outlook}

\section{Model Design}

In general, \acrshort{resnet}-18 is a relatively small and compact neural network that has limited ability to learn more complex functions. Other models like VGG net or AlexNet could potentially benefit more from data augmentation \cite{Gessert.2019, Majib.2021, WeiYu.2016}. Other hyperparameters, the loss-function or pruning algorithms might help the model to generalize better.

Since our experiments were strongly influenced by the diversity and bias of the data set, it may be useful to test whether the collected results apply equally to other data sets. The use of higher resolution images may affect the experimental results as well. Comparable data sets may also be used for transfer learning to finetune the model and address unanswered questions.

To conduct our experiments, we apply relatively simple criteria to select the model from the training process to run on our test set. The main criterion for early stopping was the validation performance with the \acrshort{mcc} score. As another criterion, we tested the comparison between the validation and the test performance. These did not lead to meaningful results in our experiments, such that only models of manually selected epochs could be compared. In the future, more advanced algorithms could be applied for choosing the right model with early stopping. This would inevitably lead to improved efficiency due to the saved computing time in the training process and increased performance. Stochastic weight averaging is one example to reach higher performance of well-tuned models.

\section{Data Augmentation}

Artificially recreating complex artifacts to augment data is a very complex and cost intensive process. For this reason, it is only possible and useful to imitate them realistically to a certain degree. In this work emulated transformations were built to better generalize the model. However, for various reasons, these methods did not have a positive impact on the test results when trained with the \acrshort{ivoct} dataset and did not help to generalize the model. Since tunica variation and artificial blood speckle did not deteriorate the performance as much, there might be potential in continuing further development. An unexplored method is the use of \acrshortpl{gan}, which can better extract the properties of features and artifacts, and thus could potentially achieve much more realistic replicas of \acrshort{ivoct} images. Antoniou \cite{Antoniou.11122017} and Odaibo \cite{Odaibo.2182019} did further investigations with a detailed focus on \acrshort{oct} imaging.

Elastic deformations could be improved by excluding certain mostly static or unimportant sections like catheter reflections from them leading to a more realistic output \cite{Simard.2003}. For example, it could be avoided that circle edges are deformed in the cartesian representation. In addition, a more realistic image could be generated by applying a uniform deformation after the cartesian transformation, which in turn could have a performance enhancing effect.

Various tests on adding noise show no or negative effect on the \acrshort{mcc} and \acrshort{bacc}. Automatic, conditional and more accurate assessment of the realistic noise distribution could make a more accurate recreation possible, so that the model can benefit from the noise filters. 

Future studies may determine in detail if and how much the combination or concatenation of different \acrshort{da} techniques affects the overall models peak performance. For instance it may be useful to further investigate whether vertical shifting during training leads to improvement or deterioration when combined with horizontal shifting. Further investigation may give insights in the ability of generalization when training on vertically shifted images.

In general, it is questionable whether all techniques with a positive effect still offer added value in the finetuning of the model. This question arises especially with techniques that generate images that deviate strongly from real images.
