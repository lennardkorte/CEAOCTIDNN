\chapter{Theoretical Concepts}

In the following fundamental concepts of the underlying experiments are described and explained in detail. Beginning with the general concepts of image acquisition with \acrshort{ivoct}, we continue by laying out how \acrshort{dl} models work. It is explained how the behaviour and performance of utilized \acrshortpl{resnet} may be interpreted. In addition to that, an overview of frequently used \acrshort{da} techniques is provided in the context of \acrshort{dl} with \acrshort{ivoct} data.

\section{Optical Coherence Tomography}

\acrshort{oct} is a medical imaging technique for translucent or opaque materials. It works by generating an interference pattern between light traveling through the sample arm and light propagating in the reference arm of an interferometer to measure the time-of-flight of light returned from tissue \cite{AdamM.Zysk.2007, Drexler.2008}.

In an early stage \acrshort{oct} images were applied to display retinal structures, but later it was also employed in diverse applications like ophthalmology, where it is used to obtain detailed images from within the retina, and optometry for painting analysis \cite{AdamM.Zysk.2007, Drexler.2008}. Oncology and dermatology are other common areas where \acrshort{oct} is used primarily for cancer detection \cite{Welzel.2001, AdamM.Zysk.2007}. In cardiology it helps to diagnose and treat coronary artery diseases \cite{HiramG.Bezerra.2009}. Technologies like medical ultrasonography, confocal microscopy, magnetic resonance imaging and \acrshort{oct} are differently suited for morphological tissue imaging. They primarily differ in resolution and depth of view. \Acrshort{oct} has achieved a high micrometer resolution (less than \( 10 \upmu \)m axially and \( 20 \upmu \)m laterally) \cite{RighabHamdan.2012, GuillermoJ.Tearney.2012, HiramG.Bezerra.2009, KostadinkaBizheva.2017}. It allows a penetration depth of a few millimeters below the surface in biological tissue \cite{AdamM.Zysk.2007, Drexler.2008}. This is because the proportion of escaping light without scattering becomes too low in deeper sample layers so that it can be extracted \cite{Bille.2019, Drexler.2008}. Other key advantages are the ability to rapidly acquire live images directly without preparation or direct contact to the subject. Furthermore \acrshort{oct} is non invasive and non ionizing \cite{Hee.1995}.

\subsection{Principle}

\acrshort{oct} employs near-infrared light with long wavelength generated by super-luminescent diodes, which allows it to penetrate into the scattering medium. The architecture illustrated in \cref{fig:octarchitecture} is based on low-coherence interferometry (Michelson interferometer) owing to the use of light with broad bandwidths \cite{Drexler.2008, HiramG.Bezerra.2009}. The optical setup typically consists of the components described as follows. Initially, light from the source is split into two beams, first the sample arm targeted at object of interest, and second, a reference arm with a mirror. The combination of light reflection from both arms generates an interference pattern. Depending on the sample properties, areas with greater light reflection will create greater interference and vice versa. Interferometry allows direct detection of reflections, considering that the speed of light is too fast to measure it directly. The low coherence characteristic of the source light will ensure that reflections outside the coherence length will not interfere \cite{Bille.2019}. Measuring reflection intensities enables the location of structures in spacial dimension of the axial depth scan (\Gls{a-scan}). When plotting an \Gls{a-scan} against the additional time dimension, we refer to it as \Gls{m-scan}. By adjusting the sample beam position we combine multiple \Glspl{a-scan} to a series resulting in a cross-sectional tomogram referred to as \Gls{b-scan} (\acrshort{2d}) or \Gls{c-scan} (\acrshort{3d}), see \cref{fig:scanpol}. \cite{Drexler.2008, Bille.2019}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../figures/oct/architecture_plain.pdf}
  \caption[\acrshort{oct} architecture]{Basic architecture of an \acrshort{fd-oct} system with a Michelson interferometer \cite{Drexler.2008} at its core. The sample arm has been extended by a probe guided by the catheter and directs its beam on to a tissue \cite{Bahoshy.2022}.  }
  \label{fig:octarchitecture}
\end{figure}

Time domain \acrshort{oct} (\acrshort{td-oct}) image acquisition is highly limited in recording speed due to its scanning rate. By moving the mirror and thus changing its path length in the reference arm a reflective profile of the sample can be scanned \cite{Drexler.2008}. The later introduced frequency domain \acrshort{oct} (\acrshort{fd-oct}) with a fixed mirror offers higher sampling rate and increased sensitivity \cite{Stifter.2007}. \acrshort{fd-oct} resolves different axial positions based on the interference spectrum acquired with spectrally separated detectors. An increasing number of detection elements reduces losses and signal to noise ratio proportionally. While the scanning range is limited by parallel detection of multiple wavelength ranges, the spectral bandwidth defines the axial resolution. \cite{Gonzalo.2010} We further distinguish between the two specialisations: spectral domain \acrshort{oct} (\acrshort{sd-oct}) and swept source \acrshort{oct} (\acrshort{ss-oct}). \acrshort{sd-oct} utilizes a broad light source with a spectrometer measuring all echo times simultaneously. Echo intensity distribution is resolved in space rather than in time \cite{Kalkman.2017}. An \Gls{a-scan} is made up by the accumulation of all axial measurements of the echo time delay. Special dispersive detectors are used to separate wavelengths. \cite{Bille.2019} In contrast \acrshort{ss-oct} encodes optical spectral frequency generated by a swept source laser. This enables the spectral measurement over time acquired by a spectral scanning source. Other known approaches are line field \acrshort{oct} using a broadband laser and line detection camera \cite{ArnaudDubois.2018, Ruini.2021} and full field \acrshort{oct}, where images are acquired orthogonal to the light beam of illumination \cite{E.Beaurepaire.1998}.

Subsequently, we describe the axial ranging with low-coherence interferometry analogous to Drexler \cite{Drexler.2008}. The following abstract will give an introductory overview of the most fundamental concepts in \acrshort{oct} technology. They have been described in papers by Billie and Drexler in detail \cite{Bille.2019, Drexler.2008}. Assuming a known spectrum of the light source, with wavenumber \( k \) and an angular frequency of the light source, we can determine the samples reflective properties. The electrical field at time \( t \) and distance \( z \) with field amplitude \( s(k,w) \) is defined as
\begin{equation}
    E_i = s(k,w)e^{i(kz-wt)}.
    \label{eq:energylightsource}
\end{equation}
The wavelength-independent beam splitter with a ratio of 0.5 ideally splits the source light into two equal parts. With a reflector distance to the beam splitter of \( z_R \) and a electric field reflectivity of \( r_R \), the returned energy can be described as:
\begin{equation}
    E_R = \frac{E_i}{\sqrt{2}}r_R e^{i2kz_R}.
    \label{eq:energyreflector}
\end{equation}
The sample has a depth dependent and continuous electric field reflectivity profile \( r_s(z_s) \). Note that \( z_s \) denotes the pathlength variable from the beam splitter along the beam axis. The reflected energy can be described with a convolution (\( * \)) by:
\begin{equation}
    E_S = \frac{E_i}{\sqrt{2}} \left[ r_S(z_S) * e^{i2kz_S} \right]
    \label{eq:energysample}
\end{equation}
For illustrative purposes we describe the sample reflection properties as a layered model \( r_S \). Thus, we abstract it as series of \( N \) discrete real delta functions of the form
\begin{equation}
r_{\mathrm{S}}\left(z_{\mathrm{S}}\right)=\sum_{n=1}^{N} r_{\mathrm{S} n} \delta\left(z_{\mathrm{S}}-z_{\mathrm{S} n}\right)
\label{eq:layered}
\end{equation}
Furthermore we have \( r_{Sn} \) as electric field reflectivity and \( z_{Sn} \) as pathlength from the beam splitter at layer \( n \). After the returning fields passed through the beamsplitter a second time, their power is halved. They interfere at the square-law detector that generates a photocurrent according to its responsivity \( \rho \):
\begin{equation}
    I_D(k,w) = \frac{\rho}{2}\bigg\langle \bigg| \frac{s(k,w)}{\sqrt{2}}r_R e^{i(2kz_R-wt)} + \frac{s(k,w)}{\sqrt{2}} \sum^N_{n=1}r_{Sn}e^{i(2kz_{Sn}-wt)} \bigg|^2 \bigg \rangle .
    \label{eq:msf}
\end{equation}
Taking into account that a detector in praxis is not able to detect the temporal angular frequency \( w \) due to its response time, we can neglect the temporal invariant terms. With the help of the Eulers rule and by substituting the power spectral dependence of the light source \( S(k) = \langle \left| s(k,w) \right|^2 \rangle \), the equation can then be rewritten as a sum of three components:
\begin{equation}
\begin{aligned}
    I_D(k) & = \frac{\rho}{4} \left[ S(k) \left( R_R + \sum^{N}_{n=1} R_{Sn}\right) \right] \text{DC Terms,} \\
           & + \frac{\rho}{2} \left[ S(k)\sum^N_{n=1} \sqrt{R_R R_{Sn}} \left( \cos [2k(z_R - z_{Sn}] \right) \right] \text{Cross-correlation Terms and} \\
           & + \frac{\rho}{4} \left[ S(k)\sum^N_{n \ne m = 1} \sqrt{R_{Sn} R_{Sm}} \left( \cos [2k(z_{Sn} - z_{Sm}] \right) \right] \text{Auto-correlation Terms}
    \label{eq:untuitiveform}
\end{aligned}
\end{equation}

First, the DC term depends on power reflectivity of the mirror and is proportional to the source wavenumber spectrum. Sample reflectivity information are provided by the cross-correlation term. It depends on the pathlength difference between sample reflectors and reference arm, as well as the light source wavenumber and contains the desired reflection profile information. Additionally the often comparatively small autocorrelation term represents artifacts created by interference occuring between sample reflectors. 

Note that the light source spectrum \( S(k) \), characterized by the wavenumber bandwidth \( \delta k \), is the inverse of the Gaussian-shaped coherence function \( y(z) \) characterized by the coherence length \( l_c \). The coherence length is a function of the light source bandwidth. \cite{Kalkman.2017}
\begin{equation}
l_{c}=\frac{2 \sqrt{\ln (2)}}{\Delta k}=\frac{2 \ln (2)}{\pi} \frac{\lambda_{0}^{2}}{\Delta \lambda}
\label{eq:wavenumberlength}
\end{equation}

\subsection{Image Reconstruction}

The raw detector signal can be reconstructed by first subtracting the known sensor specific offset and removing the DC term, which can be determined before actual measurement. In \acrshort{td-oct} the wavenumber-dependent detector current is captured on single receiver. Thus we obtain the result by integrating at each reference beam length \( k_Z \) over all k. In comparison, \acrshort{fd-oct} detects the spectral signal  \( I_D(k) \) and acquires the depth information by applying the Fourier transformation. The inverse Fourier transform should map a signal from wavenumber \( k \) to physical distance \( z \), but spectral decomposition is not necessarily linear in k-space. Therefore interpolate the wavenumber to new coordinates, also referred to as dechirping. The spectrum is linearized by mapping to corresponding non-linear and equidistant \gls{interpolation} intervals. We multiply the signal with a Hann-window to proportionally reduce the lateral deflections minimizing spectral leakage (apodisation). For interpretation as image, the complex magnitude or depth of the interferometric signal is considered, that is often logarithmically compressed. By applying the inverse Fourier transformation to the source spectrum, we obtain the desired depth information. This so called \acrshort{oct} \Gls{a-scan} with grey-scaled intensity data represents the scanned structures. The resolution is given by the coherence length along the depth axis. Laterally, the shape of the light beam is determined by the optical setup. The complex valued signal can be represented by normalizing the complex valued signal to the amplitude
\begin{equation}
\left| i(z) \right| = \sqrt{\operatorname{Re}^{2}\{i(z)\}+\operatorname{Im}^{2}\{i(z)\}}.
\end{equation}

\subsection{IVOCT}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=11cm]{../figures/oct/ivoct_plain.pdf}
    \caption[\acrshort{ivoct} catheter in vessel]{Cross-sectional view showing a probe inside a catheter with a rotational pull-back as part of an \acrshort{ivoct} system. Between every A-scan, the probe rotates by an angle of \( \beta \). Optimally smaller angles allow for a higher lateral resolution of the image (for illustrative purposes here transformed to the cartesian coordinate system).}
    \label{fig:ivoct}
\end{figure}
Cardiovascular or intravascular \acrshort{oct} is a catheter based imaging technique of in vivo coronary arteries and deployed stents (see \cref{fig:octarchitecture}). It serves as a special form of \acrshort{oct} and was established as a high definition alternative to intravascular ultrasound with a slightly shorter field of view \cite{IkKyungJang.2002, GuillermoJ.Tearney.2012}. Semi-automated imaging analyses permits acquisition of a number of accurate measurements like stent apposition and neointimal thickening that can assist cardiologists while planning or executing interventions \cite{HiramG.Bezerra.2009}. Since coronary heart disease is one of the most common treatable and preventable causes of death, there is a strong demand for reliable treatment methods. \Acrshort{ivoct} can be used by experts to detect and characterize lesions and plaque deposits on the arterial wall. In practice hundreds of images are acquired in each recording raising the demand for automatic decision support for plaque detection. \Acrshort{ivoct} requires a single fiberoptic wire at the sample arm that emits light from the source and records its reflection. \cite{HiramG.Bezerra.2009} The light is guided through the probe which itself is placed inside the catheter with 0.8 to 1.0 mm diameter \cite{HiramG.Bezerra.2009}. At the tip of the probe is a prism positioned that redirects the beam focused with a given angle directly on the object of interest, i.e. an inner vessel wall. The axial resolution is determined by the light source wavelength of \( 1,250 \text{ to } 1,350 nm \) \cite{HiramG.Bezerra.2009}. Lateral resolution is determined by the spot size focused with the spot arm lens (see \cref{fig:ivoct}). The wire simultaneously rotates while being pulled back along the artery at calibrated distances. We note that only the speed of light (\(3 \times 10^8 m/s\)) enables accurate measurements when concurrently moving the wire in space with a comparatively slow speed \cite{HiramG.Bezerra.2009}. A frequency domain \acrshort{oct} system can reach pullback speeds of up to 20 millimeters per second and a rotation rate of up to 200 frames per second \cite{HiramG.Bezerra.2009}. With a penetration depth of one to three millimeters, multiple axial \Glspl{a-scan} can be continuously acquired by reflections recorded from the vessel wall (assuming a blood free environment) \cite{Gessert.2018}. Recent versions of \acrshort{ivoct} make use of \acrshort{fd-oct} (instead of \acrshort{td-oct}) with a fiberoptic coupler similar to a beam splitter. \Acrshort{fd-oct} enables us to circumvent the limitations of generating a fast optical mirror delay. Thus we can reach a significantly higher imaging frequency with up to 100 frames per second. \cite{HiramG.Bezerra.2009, Gessert.2019}

\section{Machine Learning}

\Acrshort{ml} is a subset of artificial intelligence. It has been around since the 1950s, but only the low-cost as well as high-performance computing power and mass storage capabilities of recent years have made its use affordable. Along with this comes the increasing ease of collecting large amounts of data and the proliferation of data science. The broad field of \acrshort{ml}, generally focuses is on predicting an outcome based on the available data. It automatically maps an output to an input. \Acrshort{ml} algorithms can save huge amounts of resources like time and money by solving specific tasks. There are other areas where automated information extraction and classification as well as \gls{regression} can help to make processes more efficient. It can create new opportunities or substitute previous operations in an inestimable number of ways.

\subsection{Universal Approximators}

In contrast to rule-based decision making analysis \acrshort{ml} algorithms excel at tasks for which functions are hard to define manually and patterns can not easily be spotted by human assessment, even professionals. As a form of universal approximators they can be utilized when patterns and the related decisions depend on a tremendously high number of input variables. According to universal approximation theory \acrfullpl{nn} can approximate any type of function as long as enough nodes or layers are available \cite{Barron.1993}. State-of-the-art \acrshortpl{nn} offer the possibility to approximate many functions accurately even with a limited number of nodes. This opened a completely new field of use cases for seemingly intelligent and automatic computer programs with practical implications of \acrshort{ml} across multiple industry sectors, including healthcare, insurance, marketing, financial technology, and more.

\subsection{Classifiers}

Classification predictive modelling is the process of estimating a mapping function from input variables to its output variables. A classifier in \acrshort{ml} automatically categorizes data into one or more classes. It is an algorithm itself defined by the rules used by machines to classify them. On the other hand the classification model is the classifiers product. The classifier is used to train the model, which ultimately categorizes new data, by utilizing training data to understand the relations of input variables to the class. Classifiers allow users to constantly tailor them to changing needs in order to adapt to new circumstances. There are several kinds of classifiers, in example: Support Vector Machine, Random Forest, Decision Tree, and finally the most famous one, artificial \acrshort{nn}s. We differentiate between (semi-)supervised and unsupervised classifiers.

\subsection{Semi-/ Un-/ Supervised Learning}

In \acrshort{ml} there are two basic approaches, supervised and unsupervised learning. A model in supervised \acrshort{ml} learns from the labels of the training data over time. The prediction task is called a \gls{regression} problem, if the result is a numerical measurement of quantity mapping \( n \in \mathbb{N} \) inputs to a real valued output \( f: \mathbb{R}^{n} \to \mathbb{R} \). Otherwise we refer to a classification problem. Then the function \( f: \mathbb{R}^{n} \to \mathbb{N}_{0} \) maps to one of the \( m \in \mathbb{N}_{\geq 2} \) different classes. The output is of a single natural number \( i \in \{ 0, \dots, m-1 \} \) indicating the classes index. Another common form is the one-hot encoding vector \( \mathbf{y} = g(\mathbf{x}) \), that is generated from the algorithm, given by the function \( g: \mathbb{R}^{n} \to \mathbb{N}_{0} \), defined as follows:
\begin{equation}
    y_i = 
    \begin{cases}
        1: \textnormal{if } x \textnormal{ in class } i \textnormal{,} \\
        0: \textnormal{else,}
    \end{cases}
\label{eq:onehotmatrix}
\end{equation}

Unsupervised models assess and discover hidden patterns without the need of human intervention, e.g. labeling. They are used for three main tasks: clustering, association and dimensionality reduction. Semi-supervised learning involves both a relatively small number of labeled and often a significantly larger amount of unlabeled examples. To make use of this mixture, specialized algorithms are required. It is particularly useful when you have a high volume of data and it is ideal for medical images where a small amount of training data can lead to a significant improvement in performance. Especially in case of a small amount of marked data, this method is of importance.

This work refers to subsequent classification as part of the supervised learning category.

\subsection{Deep Learning}

In \acrshort{dl} algorithms are developed and modeled after the human brain. The development is traditionally a two-stage process. The first stage includes training and validation. In training phase the algorithms are applied using historical example data with known outcomes deriving and uncovering patterns between its features and the target variable to tune the models parameters developing the models value and leading to an overall performance improvement.

The second step is scoring, a.k.a. testing. Scoring is a key component of understanding \acrshort{ml} model outcomes. A new data set from the same origin is applied to the previously trained model. The outcomes are in form of probability scores that indicate to what extend a sample belongs to a specific class, which can then be used for classification purposes. The final step is performed by a specific rule, e.g. a threshold or the highest probability for multi class classification. For every label \( X \) in a new data set assumed as true, there exists a prediction \( X \) by the model, which has been derived by the outcome variable from the model. Each value is assumed to be in \( { 0, \dots , N } \), where \( N + 1 \) is the total number of classes. In this work we concentrate on the most common case, a two class classification problem as opposed to the multi class classification problem.

\section{Neural Networks}

\begin{figure}[hbt]
    \input{./figures/neuronal_network.tikz}
    \caption[Layered neural network]{\acrshort{nn} connected to a layered system of neurons. Three layers connect the input variables from left (input layer with \( n \) neurons) via \( l \) hidden layers (for simplicity here only one) to output variables computed by \( k \) neurons in the output layer. \cite{Neutelings.2021}}
    \label{fig:neuronalnetwork}
\end{figure}
A \acrshort{nn} is similar to the artificial human nervous system. It is designed to receive information, process it and return the information in a desired format. It refers to a system of either synapses in the real world or neurons in an artificial sense. Multiple neurons in an artificial \acrshort{nn} executed in parallel are aggregated into layer (see \cref{fig:neuronalnetwork} that may perform different transformations. Outputs from the previous layer are the inputs for the following attached layer. We differentiate between input layers, hidden layers of arbitrary multiplicity and output layers. The input layer takes given data in form of variables to be analyzed and forwards them through the hidden layers. Eventually the output layer combines previous variables to conclude a classification or \gls{regression} result. The hidden layer is exclusively linked to other layers internally.

\subsection{Neuron}

\begin{figure}[hbt]
    \input{./figures/neuron.tikz}
    \caption[Neuron]{Abstraction of a neuron with activation function applied to sum of weighted and biased variables. Variables are represented as small circles (variables \( a^{l-1}_1 - a^{l-1}_L \) left side and bias \( b^l_j \) at the top center), the activation output \( a^l_j \) on the right. \cite{Reading.2019}}
    \label{fig:neuron}
\end{figure}
In artificial \acrshort{nn}s, a neuron forms a mathematical function (see \cref{fig:neuron}, that can be concatenated with others and is traditionally defined as follows. At first, all the real values \( a^{l-1}_k \) from the \( k^{\text{th}} \) input connection to the neuron are multiplied by their associated weight \( w^l_{jk} \). \( L \) denotes the number of layers in the network and \( K \) the number of input connections. The specific bias for the \( j^{\text{th}} \) neuron in the \( l^{\text{th}} \) layer is then added to the weighted sum of all products resulting in the weighted input \( z^l_j \). We finally apply an activation function \( \sigma \) to the weighted input. The overall output is also referred to as activation \( a^l_j \). The bias provides each neuron in every node with a trainable constant \( b^l_j \) and allows to shift the activation function on the x-axis. Each connection has its own weight that can be adjusted together with the bias during training (see \cref{eq:activation}). We observe, that the activation for the \( l^{\text{th}} \) layer is related to all activations of the \( (l-1)^{\text{th}} \) layer.
\begin{equation}
    a^l_j = \sigma(z^l_j) = \sigma\left(\sum_k w^l_{jk} a^{l-1}_k + b^l_j\right).
    \label{eq:activation}
\end{equation}

\subsection{Activation Functions}

An activation function basically decides whether the information given by its input values are relevant or not and activates or deactivates the neuron accordingly. The choice of the right activation function is essential for the application of a \acrshort{ml} model. In practise many mathematical properties play a crucial role in real world application, influencing factors as e.g. computational costs or the ability of a model to approximate a special function based on given data. A back propagation algorithm traditionally uses gradient descent, that requires the derivative of the activation function to learn the weights and biases of a neural network from the gradient loss (see backpropagation section). For this reason the functions that can be used as an activation functions in \acrshort{nn}s have to be monotonically increasing, differentiable and continuously defined everywhere.

The simplest kind of activation function is the linear activation, where no transformation is applied at all, but it does not allow the network to learn complex mappings. Similarly the binary step function with \( f(x)=1 \text { If } x>0 \text { else } 0 \text { If } x<0 \) can be used for classification in \gls{regression}. Next to that we have the linear activation function \( y = m \times z \), where \( m \) is a constant variable. These approaches are not suitable to train complex \acrshort{nn}s due to their constant derivatives and wide range of activations, which could blow up easily.

\begin{equation}
S(x)=\frac{L}{1+e^{-k(x-x_0)}} \textnormal{, for } L=1, k=1, x_0=0
\label{eq:sigmoid}
\end{equation}
Rather promising approaches are traditionally provided by the \( \tanh \) and the sigmoid function \cref{eq:sigmoid}. The sigmoid s-curve used in the 1990s, is a special case of the logistic function and transforms input values from the real numbers to a range between zero and one. Later the \( \tanh \) function turned out to produce slightly better performing models in praxis than the sigmoid function, even though it maps to a range between minus one and one. Sigmoid function and tanh are both continuous and differentiable. Unfavorably both methods suffer from the same phenomenons. They are only very sensitive around their midpoint of their input and they saturate for small and large values, meaning they snap to zero or one when converging to them. \cite{Goodfellow.2016}

\begin{figure}[hbt]
\centering
\begin{subfigure}[b]{0.4\textwidth}
    \input{./figures/tanh.tikz}
    \caption[Hyperbolic tangent]{Hyperbolic Tangent.\\\hspace{0.4\textwidth}}
    \label{fig:tanh}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
    \input{./figures/relu.tikz}
    \caption[Rectified linear activation function]{Coordinate system illustrating rectified linear activation function.}
    \label{fig:relu}
\end{subfigure}
\caption[Comparison tanh and ReLU]{Line Plot comparison of Hyperbolic Tangent (a) and Rectified Linear Activation (b) for negative and positive inputs. For illustration purposes the  Hyperbolic Tangent has been multiplied by a factor of four.}
\end{figure}
Most widely used piecewise linear function within hidden layers is referred to as \acrfull{rel} given as \( f(x)=x \text { If } x>0 \text { else } 0 \text { If } x<0 \). The node implementing its function is the \acrshort{rel} unit, in short: \acrshort{relu}. We note that the derivative of zero inputs is assumed as zero as well, even though it is strictly speaking not differentiable everywhere. Experience shows that gradient descent still performs well (see \cite{Goodfellow.2016}). Because the rectifying function is linear for values greater than zero, it has a many desirable properties when training with backpropagation. The rectifier function is less computational expensive and sparse in representation compared to tanh and sigmoid function, allowing an accelerated us of \acrshort{nn}s in routine development. A neural network with linear or close to linear activation functions are easier to train and avoid the vanishing gradient problem. This opens up the possibility to also train deeper networks and optimize hardware further. \Acrshort{relu}s therefore became the practical default activation function nowadays. To reduce generalization error we apply L1 or L2 penalty to the transformed activations, which hinders them to grow arbitrarily large. \cite{Glorot.2011} \Acrshort{relu} also allows fragile (or "dying") gradients, leading to units never activating and learning with initial values of zero to be slow. Thus initial values of 0.1 or 1.0 are preferred, but they can not avoid this phenomenon completely. \cite{Maas.2013} Leaky \acrshort{relu} defined similar to \acrshort{relu} tries to tackle the problem of dying \acrshort{relu}. It differs in that small negative values are allowed when input values are less that zero. It has a constant, small and negative slope for input values below zero. Nevertheless it lacks behind sigmoid and tanh when learning functions with high complexity. Next to other activation functions like Exponential Layer Unit (ELU), Parametric \acrshort{relu} (P\acrshort{relu}) or Maxout it is one of the most common methods where we have challenges like sparse gradients often occuring \acrfullpl{gan}. 
\begin{equation}
   \sigma(\vec{z})_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}}}
\label{eq:softmaxfunction}
\end{equation}
A softmax activation function, also know as normalized exponential function (see \cref{eq:softmaxfunction}) is mainly used to form output layers for classification problems. The numerator is always the exponential function of the variable \( z_i \) ) for which the corresponding softmax value is to be calculated. It sums up the exponential function of all \( z \) ) components. As opposed to the sigmoid function it takes all other variables ( \( z_i \) ) from the previous layer into account and weighs them such that all outputs sum up to one. Note that the sigmoid is just a special case of the binary softmax function where all but one variables are set to zero. It has to be mentioned that the returned "probability" values have no real meaningful and informative value with regards to actual probabilities. Still, the transformation can be easily used in example for \gls{logistic-regression} models to build different layers of \acrshort{nn}s outputting one-hot-encoded (see section on supervised learning) sequence with the help of argmax function. For binary models choosing two neurons (with softmax) in contrast to one neuron (with sigmoid) can be effective for training and classification, but may be superfluous.

\subsection{Training and Backpropagation}

In \acrshort{ml} we differentiate between forward and backward Ppropagation. Forward propagation is referred to as feeding a network with data and computing the function that returns a prediction. Backward propagation (or backpropagation) on the other hand is the process of adjusting the weights and biases based on the difference between a prediction and the actual result.

Training begins with assigning the weights and biases. For initialisation there are mainly three options. One can initialize weights and biases with all zeros, which would serve almost no purpose, since we would face the problem of braking the symmetry \cite{Thimm.1995}. Second we can simply initialize the model with random weights and biases. The third option is to take the weights and biases from a different pre-trained model. In this case the training phase can be subdivided into pre-training and fine-tuning the weights and biases with the actual data to reduce the error as much as possible. Pre-training can be performed on a different data set. The purpose of pre-training a network on a different data set first is a faster convergence to its optimal values and a better performance when fine-tuning on small data sets. In general, pre-training only makes sense when the data sets differentiate only slightly, e.g. both hold images of similar nature. The bigger the gap, the less effective it is. Often pre-trained networks are provided publicly for specific purposes and such pre-trained networks give us the initialisation values \cite{TorchContributors.2017}.

The model is applied to the data set and new weights and biases are then computed with the goal of reducing the error value across the training set to a minimum. Backpropagation describes how the weights and biases in a neural network change the cost function. A cost function is a technique or measure to evaluate how wrong a models performance is. There are several different commonly used cost functions available, e.g.: mean (absolute) squared error (\acrshort{mse}) loss, hinge embedding loss and negative-likelihood loss. One of the most popular cost functions refers to as the cross-entropy loss, where \( L \) denotes the total number of layers and \( N \) the number of training examples. We also define the corresponding desired output \( y(x) \) as well as the activation \( a^l_j \) just like in the previous section about neurons. We average over the a sample set resulting in the cost function
\begin{equation}
    C = - \frac{1}{N} \left( \sum_{i=1}^N y(x) \cdot \log(a^L(x)) \right)
    \label{eq:crossentropy}
\end{equation}

Cross-entropy is a better measure than \acrshort{mse}, because the decision boundary in a classification task is large. \Acrshort{mse} is the right loss for \gls{regression}, but does not punish misclassifications enough, because the distance between predicted values is small. The inner product is denoted by "\( \cdot \)". Further we assume that \( C \) can be computed by the average derivatives over all training examples \( C_x \) and it can be written as a function of the last layers outputs \( a^L \). \cite{Nielsen.2015} \cite{Dreyfus.1990}

For backpropagation we need to compute the partial derivatives of the cost function \( C \) with respect to the weights and bias first. The backpropagation algorithm can be described in five steps. In the first step we set the activations for the input layer. Next, we feedforward the values by computing the activations for the neurons in all following layers. In step three we compute the error vector \( \delta^L \) for the output layer.
\begin{equation}
    \delta^L = \nabla_a C \odot \sigma'(z^L),
\end{equation}
The error is denoted as \( \delta^i_j \) for the \( i^{\text{th}} \) neuron in the \( j^\text{th} \) layer defined as \( \partial C / \partial z^l_j  \). Here \( \nabla_a C \) is defined by the vector of partial derivatives of \( C \) with respect to \( a^L_j \). The two terms on the right multiplied by the Hadamard product measure "how fast the cost is changing as a function of the \( i^{\text{th}} \) output activation" multiplied by how fast "the activation function \( \sigma \) is changing at \( z^L \)". \cite{Nielsen.2015} In the fourth step we compute the error of previous layers \( l=L-1, L-2; \dots, 2 \) in terms of the error of layer \( L \) as follows:
\begin{equation}
    \delta^l = ((w^{l+1})^T\delta^{l+1}) \odot \sigma'(z^l).
\end{equation}
By repeatedly applying this equation to previous we propagate the error backwards through the network and get errors for every weight. This is possible due to the chainrule. Lastly we compute the rate of change to the costs in the network with respect to any bias with: \( \delta^l_j = \partial C / \partial b^l_j \).
\begin{equation}
    \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.
    \label{eq:weightserror}
\end{equation}
In addition to that we compute the partial derivatives with respect to the weights \( w^l_{jk} \) (left side of the equation) by multiplying the error of the neuron output with the activations of the neurons input (see \cref{eq:weightserror}). \cite{Nielsen.2015}

Practically the errors are computed for sample batches of different sizes, then averaged and applied all together. The batch size has an impact on the models performance. For a higher batch size the error fluctuates less, but also fails to put rarely occuring features into account. Most common and effective batch sizes in image classification are two to the power of 4 to 10. \cite{IbrahemKandel.2020} In the following optimization algorithms or \glspl{optimizer} iteratively try to find a local minimum of a function. For this it is assumed that the function is convex and differentiable. Stochastic gradient descent is often preferred over batch and mini batch gradient descent, because it is faster and is able to take rare features into account. The Adam optimizer solves issues of the gradient descent algorithms, like finding the right \gls{learning-rate}, dealing with rare features in sparse data and getting stuck in suboptimal local minima. As one of the class of adaptive optimizers it automatically tunes the \gls{learning-rate} value while training. Generally, the Adam optimizer performs best for \acrshort{cnn}s \cite{VaniS..2019}. In \acrshort{ml} the optimizers adjust the \gls{learning-rate} before calculating new weights as follows: 
\begin{equation}
    \operatorname{NC} = \operatorname{LR} * \operatorname{E} + \operatorname{M} * \operatorname{LC}.
    \label{eq:nwchange}
\end{equation}
This process makes increases training efficiency significantly. The \gls{learning-rate} (\( \operatorname{LR} \)) controls the speed of the models learning process. Then, the optimizer reduces the \gls{learning-rate}, which is dependent on the calculated gradient. An optimal initial value often lies in the order of a few digits after the decimal point. The training process generally starts with a high \gls{learning-rate} which is reduced in the process. Error (\( \operatorname{E} \) represents the difference between what the output with specific parameters in the current model is and what it should be. \cite{Erb.1993} High \gls{learning-rate} can lead to an oscillation and prevent the model from converging to an optimal solution. A lower rate may result in an unnecessary long training time. The gradient descent could get caught in a sub-optimal local minimum (see \cref{fig:gradientdescent}). There may be multiple optimal solutions as local minima. The momentum (\( \operatorname{M} \)) prevents widely fluctuating new weight changes (\( \operatorname{NC} \)) by multiplication with last weight change (\( \operatorname{LC} \)). \cite{Erb.1993} \cite{Brownlee.2017}

\subsection{Residual Networks}

As a rule of thumb the performance of a \acrshort{nn} increases the more layers we stack to each other. The general intuition behind a \acrshort{nn}s as function approximators suggests that more layers increase their overall performance. But this generalisation can be misleading, since “the deeper the better” does not always hold. In 2015 Kaiming He and colleagues have shown that performance saturates degrades after some depth due to vanishing gradient (like in VGG nets \cite{Simonyan.942014}) and other related phenomenons \cite{He.12102015}. This degradation problem results in weights and biases never updating its values, hindering the \acrshort{cnn} to learn simple functions e.g. the identity function without further ado. Problems like this can be minimized by bypassing layers in between with a residual connection introduced in the context of information highway networks in 2015 \cite{Srivastava.7222015}. This way larger gradients can be propagated directly through the skipping connections, a.k.a. identity shortcut connections (see \cref{fig:resblock}). Thus we have \( H(x) = R(x) + x \) such that the network learns the residual \( R(x) \) decisively faster as opposed to a traditional network learning \( H(x) \) \cite{He.12102015}. This allows us to train much deeper and better performing networks than before, e.g. with AlexNet \cite{Krizhevsky.2017}.
\begin{figure}[hbt]
    \centering
    \includestandalone[width=0.4\textwidth]{../figures/resblock}
    \caption[Single residual block]{Single Residual Block. \cite{He.12102015}\cite{Riebesell.2022}}
    \label{fig:resblock}
\end{figure}

In \acrshort{resnet}s we can traditionally observe residual blocks with either two or three weight layers that are skipped by a shortcut connection defining a single residual block. It has been shown that more dense shortcuts allow even deeper \acrshort{nn}s and therefore there exist residual blocks with only one weight layer.\cite{Huang.8252016}. There exist different versions of weight layers. Pre-activation layers with first a \gls{batch-normalization} and the subsequent activation function mostly give the best results \cite{He.3162016}, but other sequences are possible as well. From an intuitive perspective, we can see the skip connections as dynamic hyper-parameters that are tuned automatically. They determine how many layers are optimally used and which nodes are used during training. There are different versions of \acrshort{resnet}s, but their general architectural concept remains the same. 

\begin{figure}[hbt]
    \centering
    \includestandalone[width=0.5\textwidth]{../figures/convolution}
    \caption[Convolution]{Convolution using Cross Product. \cite{user194703.2019}}
    \label{fig:convolution}
\end{figure}
One of the fundamental concepts of a \acrshort{resnet} are convolutions layers. Having convolutional layers makes the \acrshort{resnet} a special type of \acrshort{cnn}. The basics of a \acrshort{cnn} architecture consist of a convolution, pooling, and fully connected layer. In contrast to fully connected layers they look at specific features instead and use filters (or kernels) to extract specific features. This has several reasons. Among them are computational efficiency and provable performance improvements, e.g. with regards to training speed and prediction performance. \cite{Goodfellow.2016} The two dimensional filter is multiplied with a filter-sized patch from the input data. This scalar product results in a single valued score. A convolutional layer then applies multiple overlapping convolutions systematically across the input volume returning a feature map, on which an activation function can be applied. Note that convolution here may in other contexts be referred to as flipped cross-correlation. \cite{Goodfellow.2016}

The number in the name of a specific \acrshort{resnet} (e.g. \acrshort{resnet}-18) denotes how many layers there are. Commonly a \acrshort{resnet} begins with a 7x7 convolution with a feature map of size 64, \gls{batch-normalization} (elementwise) and max-pooling operation. The 64 refers to the number of kernels (or feature maps) that are convolved with the input. With this techniques the model becomes the ability to recognize specific features from a network that has learned rich feature representations for a wide range of images. \gls{batch-normalization} is a technique that standardizes inputs to a layer batch wise. Training speed can be drastically increased, when applying it after each convolution. \cite{Brownlee.2019} Max-pooling refers to calculating the maximum value for each patch of the feature map. The convolution is padded with three zeros and the max-pooling operation has a stride of two. Stride is how far the filter moves forward in every iteration step across the data. All together this halves the input dimensions from (\( 224 \time 224 \time 1\)) to a (\( 122 \time 122 \time 64 \)) volume. Note that for simplicity this is free of multi batch dimensionality, e.g. thee channels in one image. This is often used for colorized images, but does not change general principles. The following layers (see e.g. \cref{tab:ResNet18}) are composed of several basic blocks with two operations (see \cref{fig:resblock}) as opposed to bottleneck blocks with three operations. \cite{He.12102015} Note that these operations were previously also referred to as layers. The layers here consist of multiple blocks, most of which do not change the volume size. Unlike in normal \acrshortpl{cnn}, where downsampling is done by pooling operations, the size is changed by increasing the stride of only the first operation in the layer from one to two. When size changes we use projection shortcuts with a convolutional operation instead of the usual identity shortcuts. Duplicating the number of filters, when downsampling preserves time complexity. An average pooling layer and a fully connected layer are appended at the end of the \acrshort{resnet}. An average pooling layer calculates the mean of each feature map before concatenating all the values into a \acrfull{1d} list. \cite{He.12102015}

\subsection{Metrics}

While the network evolves as well as after its development process, monitoring of processing elements, values and overall performance enables us to make fundamental decisions about parameters like its training time, overall topology and learning parameters. One way to make investigations on \acrshort{ml} models based on observations is to create a \gls{heat-map}. When doing a sufficient number of tests for one model, as described below (see estimation of errors), we can compare different models and choose the most accurate one that produces the valuable insights based on computed metrics.

Performance metrics are extremely useful when evaluating and comparing the different classification models or \acrshort{ml} techniques resulting after training. To test the ability of a so called classifier, there are different metrics that prove useful for comparing the performance of two different models and analyzing the behavior of the same model by varying different parameters. In this work we concentrate on classification metrics as opposed to \gls{regression} problems, where relative errors play a more essential role.

Subsequently condition positive (P) represents the number of real positive cases in the data, whereas condition negative (N) stands for the number of real negative cases in the data. We used the following metrics to assess the segmentation performance of our models.

\subsubsection{Confusion Matrix}

The \acrshort{2d} confusion matrix (in unsupervised learning a.k.a. matching matrix) compares the instances in actual classes (here rows) with the instances in predicted classes (here columns) (\cref{tab:confusionmatrix}), so that for \( C_s \) as either the predicted or actual sum of classes holds:
\begin{equation}
    C_s = P + N
\end{equation}
As a special kind of contingency matrix it easily illustrates weather two classes are commonly interchanged with each other. We make the correct results appear in green for easy identification. Confusion matrices can in general also be used as multi-class classifiers as well \cite{Grandini.8132020}, where the diagonalized matrix at best represents a unit matrix. We will concentrate on binary classification in the following though. In total there are four types of basic confusion matrix rates. True positive (TP) results that correctly indicate the presence of a condition or characteristic whereas true negatives (TN) correctly indicate their absence. Corresponding to this, false positive (FP) results wrongly indicate that a particular condition or attribute is present and false negatives (FN) wrongly indicate their absence. The sum of all fields equals the total number of samples.
\begin{table}[h]
    \begin{tabular}{cc|c|c|c}
    \cline{3-4}
    & & \multicolumn{2}{ c| }{Predicted condition} \\
    \cline{3-4}
    & & Positive (PP) & Negative (PN) \\
    \cline{1-4}
    \multicolumn{1}{ |c  }{\multirow{2}{*}{Actual condition} } &
    \multicolumn{1}{ |c| }{Positive (P)} & \cellcolor[HTML]{ceffbc} True positive (TP)  & \cellcolor[HTML]{ffbfae} False negative (FN) \\
    \cline{2-4}
    \multicolumn{1}{ |c  }{} &
    \multicolumn{1}{ |c| }{Negative (N)} & \cellcolor[HTML]{ffbfae} False positive (FP) & \cellcolor[HTML]{ceffbc} True negative (TN) \\
    \cline{1-4}
    \end{tabular}
    \caption{Confusion Matrix}
    \label{tab:confusionmatrix}
\end{table}
The so-called confusion matrix allows a much more comprehensive analysis of a binary predictive analysis. Hereafter, we will discuss these types of analysis in detail for bi-classification systems.

\subsubsection{Classification Accuracy}

Calculating the classification \acrfull{acc} (or weighted \acrshort{acc}) is the most obvious and common technique to investigate the proportion of correct classification in a confusion matrix. We compute is by taking the arithmetic mean of all True (1) and False (0) Predictions, see \cref{eq:accuracy}. In most diagnostic cases of diseases it reduces to dividing the sum of true by the total number of classifications. To its inconvenience it yields misleading results for unbalanced data sets, because it gives more weight to instances coming from the majority class.
\begin{equation}
    \begin{gathered}
        \operatorname{ACC} = \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i  =  \frac{1}{n} (x_1 + \cdots + x_n) = \frac{TP + TN}{TP + TN + FP + FN} \textnormal{ ,}\\
        \textnormal{for } x_i \in \{\textnormal{all True (1) and False (0) predictions}\} \\
    \end{gathered}
    \label{eq:accuracy}
\end{equation}

\subsubsection{Sensitivity and Specificity}

\Acrfull{sens} or Recall refers to the probability of a positive test, conditioned on truly being positive, see \cref{eq:sensitivity} \cite{JacobYerushalmy.1947}. It is a measure of how well a test can identify true positives.
\begin{equation}
    \operatorname{SENS} = \frac{TP}{ \text{Actual True}} = \frac{TP}{TP + FN}
    \label{eq:sensitivity}
\end{equation}
\Acrfull{spec} (or selectivity) refers to the probability of a negative test, conditioned on truly being negative (\cref{eq:specificity}) \cite{JacobYerushalmy.1947}. It is a measure of how well a test can identify true negatives. \acrshort{spec} can be calculated according to the negative scheme as \acrshort{sens}
\begin{equation}
    \operatorname{SPEC} = \frac{TN}{ \text{Actual False}} = \frac{TN}{TN + FP}
    \label{eq:specificity}
\end{equation}
Higher \acrshort{sens} generally means lower \acrshort{spec} and vice versa, which is why there is usually a trade-off between them. To identify a true condition accurately the \acrshort{sens} should be high. To identify a false condition accurately the \acrshort{spec} should be high, that is 100\% in an optimal case. \acrshort{sens} and \acrshort{spec} are intrinsic measures and therefore independent of the prevalence. Because \acrshort{spec} does not consider false negatives, a negative result from a test with high \acrshort{spec} is not necessarily useful for ruling out disease. The same applies to \acrshort{sens}, which does not include false positives and therefore should not be used to detect a disease. Therefore the diagnostic power of any test is usually determined by both, its \acrshort{sens} and its \acrshort{spec} \cite{Boyko.1994}.

Miss rate refers to the \acrshort{sens}'s and fall-out to the \acrshort{sens}'s complement. Because \acrshort{sens} and \acrshort{spec} measures are correlating, a case-specific weighting must often be determined. %This trade-off is explored in \acrshort{roc} analysis described below.
This trade-off is explored in \acrfull{roc} analysis not considered in detail. 

\subsubsection{Balanced Accuracy}

By computing the arithmetic mean of \acrshort{sens} and \acrshort{spec}, we get the balanced \acrshort{acc} (\acrshort{bacc}), see \cref{eq:balancedaccuracy}. It offers the advantage that it does not suffer from unbalanced data sets.
\begin{equation}
    \operatorname{BACC} = \frac{\operatorname{SENS} + \operatorname{SPEC}}{2}
    \label{eq:balancedaccuracy}
\end{equation}

\subsubsection{Positive and Negative Predictive Values}

Positive and negative predictive values (\acrshort{ppv} and \acrshort{npv} respectively) are the proportions of positive and negative results in diagnostic tests per true positives and true negatives \cite{Skaik.2008}. For instance \acrshort{ppv} is often also called precision (\cref{eq:precision}), due to its statistical relevance. \acrshort{npv} can be computed following the same pattern. A perfect test result would have a score of 1 (for 100\%), while the worst test would give a score of 0.
\begin{equation}
    \operatorname{PREC} = \frac{TP}{ \text{Total True Predictions}} = \frac{TP}{TP + FP}
    \label{eq:precision}
\end{equation}
\acrshort{ppv} and \acrshort{npv} are not intrinsic and therefore depend on the prevalence (overall proportion of positive test results). That means a higher prevalence results in a higher \acrshort{ppv} and conversely a lower prevalence in a lower \acrshort{ppv}. The opposite is true for \acrshort{npv}. To meaningfully compare multiple test sets an equivalent prevalence is therefore inevitable. This is often achieved by normalizing the prevalence to 50\%. We have the \acrfull{fdr} and \acrfull{for} as complements to \acrshort{ppv} and \acrshort{npv}.

\subsubsection{F1-score}

The F1-score (\(\beta = 1\)) or dice coefficient (after its geometric interpretation) is a special case for the \( F_\beta\) measure defined in \cref{eq:f1score}. It is the harmonic mean of \acrshort{ppv} and \acrshort{sens}. In contrast to the \acrshort{bacc} the F1-score doesn’t take into account how many true negatives are being classified or the number of negative examples at all. "It generates a high score only if the number of true positives obtained is high compared to the other confusion matrix cells." \cite{Hoffman.2020} Therefore we only prefer F1-score over \acrshort{bacc} for imbalanced data when more attention is needed on the positives. It highly depends on which class is defined as positive. The F-measures key advantage is that it can apply additional weighting, valuing either \acrshort{ppv} or \acrshort{sens} more than the other. The lowest possible score is 0 if \acrshort{ppv} or \acrshort{sens} is zero, while the perfect value is 1 indicating a perfect classification of the system. The F-measure by Van Rijsbergen was derived so that \( F_\beta \) "measures the effectiveness of retrieval with respect to a user who attaches \( \beta \) times as much importance to recall as precision" \cite{vanRijsbergen.1979}.
\begin{equation}
    \begin{gathered}
        F_\beta
        = (1 + \beta^2) \times \frac{PPV \times \operatorname{SENS}}{\beta^2 \times PPV + \operatorname{SENS}} \\
        \Rightarrow F_1
        = \frac{2(PPV \times \operatorname{SENS})}{PPV + \operatorname{SENS}}
        = \frac{TP}{TP + \frac{1}{2}FP + FN}
        = \frac{2TP}{2TP + TP + FN}
    \end{gathered}
    \label{eq:f1score}
\end{equation}

\subsubsection{Matthews Correlation Coefficient}

Another balanced measure to evaluate the classification performance is denoted as the phi coefficient (\( \phi \)) or \acrshort{mcc}. It describes the correlation between the observed and predicted classifications and can be calculated from the confusion matrix \cref{eq:mcc} as well. A coefficient of one indicates a perfect prediction, zero is no better than a random prediction and minus one indicates the worst prediction possible. It does not yield a reliable estimate on how close the classification model is to a random prediction. The \acrshort{mcc} takes into account the balance ratios (more precisely: low precision) of the confusion matrix entries and unlike the F1-score doesn't depend on which class is the positive one, which is why it is preferred for balanced data sets. To its detriment, \acrshort{mcc} deteriorates decisively with unbalanced data sets \cite{Zhu.2020}.
\begin{equation}
    MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
    \label{eq:mcc}
\end{equation}

\begin{comment}
\subsubsection{Receiver Operating Characteristic}

\begin{figure}[hbt]
    \input{./figures/roc_space.tikz}
    \caption[Receiver operating characteristic space]{Receiver operating characteristic space with two \acrshort{roc} curves (blue and green), \acrshort{lnd} (red, dotted) and perfect classification point P\textsubscript{c} (top left).}
    \label{fig:rocspace}
\end{figure}
The \acrfull{roc} analysis provides tools to find and select optimal models independently from the class distribution. It therefore graphically illustrates the classification systems ability by plotting the \acrshort{sens} against the fall-out with different parameter settings (e.g. threshold when using \gls{regression}). We can connect those plots as points with a line in one \acrshort{roc} space. The so called \acrshort{roc} curve summarizes all the confusion matrices obtained from the tests of one binary classification system. The best possible classification system yields a point in coordinate (0,1) (perfect classification), representing a \acrshort{sens} and \acrshort{spec}  of 100\%, while random guessing yields to points on the diagonal (\acrfull{lnd}) from left bottom to right top. A classification system with points on a curve closer to the perfect classification above the \acrshort{lnd} is therefore considered better and vice versa. As we can revert every negative prediction below the \acrshort{lnd}, the best indicator of how much predictive power a classification system has is the distance to the random guess line in either direction.

By computing the area under the \acrshort{roc} curve (\acrshort{auroc}) we can compare one \acrshort{roc} to another, see \cref{fig:rocspace}. Generally an AUC closer to one indicates a better and zero the worst classification system performance. This \acrshort{roc} curve plays a central role in evaluating the diagnostic ability of tests to discriminate the true state of subjects and comparing two alternative diagnostic tasks performing on the same subject \cite{HajianTilaki.2013}. In \cref{fig:rocspace} we see that the \acrshort{auroc} is significantly larger for the classification system represented by the blue curve than for the green curve. As an additional \acrshort{roc} version fall-out is replaced by precision. This \acrfull{aupr} is often more useful in case of imbalanced data sets, because of its independence regarding prevalence. Compared to \acrshort{auroc}, classifier comparison with \acrshort{aupr} score can favour models with lower F1-scores \cite{PeterFlach.2015}.

In summary both analysis methods, \acrshort{auroc} \cite{Halligan.2015}\cite{Wald.2014} and \acrshort{aupr} score \cite{Raghavan.1989}, suffer from several drawbacks that can lead to misleading conclusions. To solve this problem Hoffman and colleagues have recently introduced a new metric that compares classifiers more clearly. The \acrshort{mcc}-F1-score is the area under curve of the \acrshort{mcc} against the F1-score \cite{Hoffman.2020}. The \acrshort{mcc} axis is unit-normalized (add one, then divide by two) so that both, the x-/ and y-axis have the range [0, 1]. This results in an optimal score with the best performance by the classification model of [1,1] in the upper right corner and a point of the worst performance in the bottom left corner with coordinates: [0,0]. For random predictions the \acrshort{mcc} gives a value of 0. This yields to a random line with unit-normalized \acrshort{mcc} equal 0.5 \cite{Hoffman.2020}.
\end{comment}

\subsubsection{General Remarks}

"Even if four-category confusion matrices are a common tool to evaluate binary classifications in supervised \acrshort{ml} and statistics, no general consensus has been reached on a unique statistical score able to informatively recap its key-message." \cite{Chicco.2021} Other common metrics like \acrfull{dor}, Jaccard coefficient, \acrshort{bacc} weighted \cite{Grandini.8132020}, bookmaker informedness and markedness \cite{Chicco.2021b} do not suit for evaluating a binary \acrshort{dl} classification system or are less informative than the measures described above and will therefore not be considered further in this study. For instance \acrfull{dor} does not take into account precision and negative predictive value \cite{Chicco.2021}. As the main disadvantage Fowlkes-Mallows index does not consider true negatives. In this work we do not consider \acrshort{bacc} weighted that keeps track of the importance of each class for imbalanced data sets. Research has also shown that the kappa measure should be avoided over \acrshort{mcc} \cite{Delgado.2019}. For this reason, we focus on the most promising and common metrics in order to get more meaningful results as well as maintain consistency and practicality for the purpose of information transfer. Confidence intervals can be used to evaluate metrics and see how precisely we can measure our metrics performance. These methods have limited meaningfulness though, when data samples are correlated to each other and training sets are small.

In \acrshort{dl}, the rule of thumb is that more data results in better performance and vice versa. Small data sets often limit the training capabilities and the resulting performance of trained. Another factor that significantly affects the performance of a \acrshort{dl} model is the quality of the data used to train it. This includes various factors such as the information content, the diversity, the independence and the preciseness of the manually determined variables (or labels). Collecting more training data is often a challenging task or not feasible for medical purposes. This is due to high costs or the limited possibilities to create or collect sample data. Also, labeling large amounts of data often involves an immense effort or cost, not to mention the professional expertise and certain knowledge that is required to label data consistently. These issues arise particularly frequently in the medical field. In order to mitigate these shortcomings and still achieve a valuable performance of the \acrshort{dl} models, \acrshort{da} is applied.

\subsection{Overfitting and Generalization}

The most important thing to do while and after training is to ensure as well as evaluate the classifiers applicability. One of the main goals is to prevent or delimit \gls{overfitting} (or variance) to its minimum. We likewise want to avoid underfitting (or bias) and achieve a certain level of \gls{generalizability} considering that the model should be similarly well applicable to data from the same domain. \Gls{overfitting} is "a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. As a result, the model is useful in reference only to its initial data set, and not to any other data sets." \cite{Twin.2021} In \acrshort{ml} it refers to high error with respect to test data as well as low prediction error with respect to training data. Overfitting occurs when a model learns details and noise of the training data that are not reflected in the test data it is applied to later, e.g. when testing \cref{fig:overfitting1}.
\begin{figure}[H]
    \centering
    \includestandalone[width=0.7\textwidth]{../figures/overfitting2}
    \caption[Sweet-spot of generalization]{ Smoothed line plot of training error (y-axis) for every epoch (x-axis), highlighting the sweet-spot from which on model performance decreases due to overfitting. \cite{user121799.2022}}
    \label{fig:overfitting2}
\end{figure}
It can be recognized by comparing the models performance with seen and unseen data. When \gls{overfitting} occurs in the training process, there may be a spot from which on error on unseen data becomes worse than before while the seen data perform distinctively better than unseen data. The goal is to create a model that generalizes well \cref{fig:overfitting2}. Generalization refers to how accurate the model performs on unseen examples compared to the ones it has been trained with. Therefore, the one goal is to find this sweet spot.

The easiest way to prevent \gls{overfitting} while training is to stop the training process when the error rate on the validation set increases. But there are several other techniques presented and used in the following work to diminish over- and underfitting while training to obtain more representative performance values.

\subsubsection{Pruning and Dropout}

\Gls{overfitting} can be promoted due to an overly complex model with too many parameters. Pruning refers to the process of removing entire groupings or layers of nodes and weight connections from the neural network to increase inference speed and decrease model storage size, while affecting metrics such as \acrshort{acc} as little as possible. We differentiate between structured and unstructured pruning. Removing layers is a simple way of pruning. A more complex example is provided by algorithms removing nodes and connections using specific criteria that maintain the performance of the model as good as possible. Furthermore dropout describes a variant that randomly deletes single nodes with a certain probability from all layers except input and output layers. In each epoch a different set of nodes is kept so that the neural network can not rely on any input node. Thus it will become reluctant to give high weights and biases to certain features and a small quantity of nodes, because it is random if they disappear. One can remove features from the training data to reduce the risk of \gls{overfitting}, but this is not feasible for a binary classification.

\subsubsection{Regularization}

\begin{equation}
    \text{Loss} = \text{Error}(y, \hat{y}) + \lambda\sum_{i=1}^{N} |w^2_i|
    \label{eq:ridgeregression}
\end{equation}
L1 and L2 regularization (after L1, L2 norm, a.k.a. lasso \gls{regression} and ridge \gls{regression}) are two other possibilities to mitigate \gls{overfitting}. Ridge \gls{regression} adds the sum of squared parameters \( w_i \) multiplied by the coefficient \( \lambda \) to the loss function, which penalizes large weights and biases of a model in order to minimize the cost function (see \cref{eq:ridgeregression}). The loss function here predicts the difference between true values \( \lambda \) and predicted values \( \hat{\lambda} \). The \( \lambda \) value is determined using \acrlong{cv}. Computationally, lasso \gls{regression} only differentiates in that it does not square the variables, but takes the absolute value instead. With this it can shrink and exclude useless variables from equations completely to zero.

\subsubsection{Cross-Validation}

\Acrshort{cv} or rotation estimation is the model validation technique assessing how well a model generalizes to predict new labels from unknown and independent data sets with the same origin. There are several different variants of exhaustive and non-exhaustive \acrshort{cv}, e.g. leave-one-out or K-fold \acrshort{cv}. Non-Exhaustive techniques do not compute all ways of splitting the original sample, mainly to limit its complexity for a rising number of data, while exhaustive ones do. To be able to train a model and at the same time meaningfully measure its performance, a data set is usually divided into three empirically separated subsets: training set, validation set and test set (or holdout). K-fold \acrshort{cv} uses different subsets of the data to train and test a model on multiple iterations. This enables a quite accurate estimation insight of the algorithms performance quality indicating characteristics like selection bias or \gls{overfitting}. The training set together with the validation set (known labels) make up the main part of a data used, which is usually around 70-90\% of the entire data set. Test data (unknown labels) make up the rest. The validation set size is traditionally in the range from 1/5 to 1/20 of the test set. Both sets are rotating in every \acrshort{cv}-step used during the training process of the algorithm to calculate its performance measure (see \cref{fig:crossvalidation}). Unknown test data remain static and unchanged. In every \acrshort{cv}-step we train a new model with the specific validation holdout, which gives us \( n \) trained models. Performance is then assessed in the scoring phase where each model is applied to the test set, such that measures can be computed from the metrics (see metrics section) independently from training. The described variant of K-fold \acrshort{cv} fulfills its purpose relatively well and at the same time keeps the computing costs within a realistically applicable range. Taking into account that estimates for small K are usually more noisy and have an upwards bias, larger k values are desirable. Because the computational burden and the leave-one-out \acrshort{cv} is too heavy, a K-value equal to five or ten may be used. \cite{Fushiki.2011} Distinguishing between test set and validation set prevents each model from \gls{overfitting} on the data it has been validated with, which leads to a decisively more accurate testing result. In the end we average all score variables creating a summarizing performance indicator representative for a model trained with the entire training set. 

\section{Data Augmentation}
In \acrshort{dl}, the rule of thumb is that an increased amount of data results in better performance and vice versa. Unlike in the natural image domain, there are far fewer training images available in various other domains. Small data sets often limit the training capabilities and the resulting performance of possibly high value models. Many parameters are undetermined in low-data regimes, which leads to poor generalisation of learnt networks. Collecting and labeling training data happens to be one of the greatest challenges when creating qualitative \acrshort{dl} models. Particularly in the medical sector, there is often a lack of professionally labeled data and anatomical images are typically very limited. Due to the immense effort and costs, collecting large data sets is often be unfeasible. Another factor that significantly affects the performance of a \acrshort{dl} model is the quality of the data used to train it. This includes various factors such as the information content, the diversity, the independence and the preciseness of the manually determined variables (or labels). Since we often have very limited to no control over these factors in medical low-data regimes, \acrshort{da} became an essential part of training \acrshort{cnn}s. In order to make simulation feasible and simple, Wong and Tanner popularized \acrshort{da} initially \cite{MartinA.Tanner.1987}. \acrshort{da} uses existing data with increased effectiveness to overcome sparsity of training data. It refers to the application and the combination of various \acrshort{da} techniques to the corresponding training data to expand as well as diversify the original training data set. \Acrshort{da} is applied in order to mitigate the above described shortcomings and still achieve a valuable performance while diminishing \gls{overfitting} of the \acrshort{dl} models by changing the images pixel values but keeping the majority of information in it. It is aimed at increasing the generalization of the resulting \acrshort{dl} model and thus achieving a more reliable and ultimately a higher overall prediction performance. \Acrshort{da} techniques and their combination have been heavily researched for natural image tasks. Even if \acrshort{da} is widely utilized in medical imaging for \acrshort{dl}, little research has been done to determine which augmentation algorithms capture \acrshort{ivoct} image features best, producing strategies for more discriminative models \cite{hussain.2018}. In the following, we focus on different \acrshort{da} techniques that have publicly been proposed at an earlier point of time to transform \acrshort{ivoct} images. For the sake of this study we differentiate between commonly applied \acrshort{da} techniques and (intravascular) \acrshort{oct} specific approaches.

\subsection{State of the Art}
Intuitively common \acrshort{da} techniques can not always be usefully applied to polar as well as Cartesian formats the same way. Different approaches for transformations can be differentiated and therefore grouped by their individual characteristics which have distinguishing effects on the images they are applied on. We group the approaches into three categories. To the first category belong approaches that are usefully applicable to the polar format, then the Cartesian format and finally we group applicable to both formats. Because \acrshort{ivoct} images are grayscaled we restrict ourselves to filters applicable to images with a single channel.

\subsubsection{Affine Transformations}

Most common approaches on polar representations are shifting \cite{Kuwayama.2019, Kihara.2019} and random flipping. For the former approach the images are shifted randomly along the \( \theta \) direction with \( s_{\theta} \in [0 px, 300 px] \). \Glspl{a-scan} are consequently shifted out along the positive direction and must be added back to the opposite site. Random horizontal flipping can additionally be applied along the temporal axis in \( \theta \) direction \cite{Devalla.2018, KunGao.2019, Kihara.2019, Gessert.2018, Gessert.2019}. Overlapping \Glspl{a-scan} need to be appended accordingly. In combination one can observe the commutative property of those random transformations. As a result, one only randomises in either shifting or flipping when combining them, as the output set and probability distribution stays the same.

In Cartesian representation images are often rotated randomly \cite{BenCohen.2018, Devalla.2018, Kuwayama.2019, hussain.2018, Kihara.2019, Gessert.2018, Gessert.2019} with \( \theta \in [ 0^{\circ} , 360^{\circ} ] \) which can cause information loss due to \gls{interpolation} for \( \theta \notin ( 0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ} ) \). Random flipping \cite{Devalla.2018} can be applied to Cartesian images in every angle due to its circular nature (undefined edges values remain static). Rotation around the center has an almost equivalent effect on the outcome, as if we apply a random shift in polar representation and convert it to a Cartesian representation afterwards. The main difference is, that it is restricted to a an angle of \( 2 \pi / \theta \), such that an extra \gls{interpolation} can be avoided. This is why one can neglect center rotation, when the mapping between coordinate systems takes place. Random flipping and rotation are not commutative. If \gls{interpolation} is neglected, the set with possible outputs also differs, if one of the two random factors is limited to a certain range of possible values. Flipping along the x and y axis is least cost efficient, because \gls{interpolation} can be neglected. One can produce approximately the same result set and probability distribution by applying the random flipping on x and y axis first and a rotation in a second step. A polar image may be rotated by an \gls{interpolation} after applying a mapping described by the matrix \( A \), where \( \theta \) is the rotation angle:
\begin{equation}
    A = 
    \begin{pmatrix}
    \cos{\theta} & -\sin{\theta} \\
    \sin{\theta} & \cos{\theta}
    \end{pmatrix}
    \textnormal{, where } \theta \textnormal{ is the rotation angle.}
\end{equation}

In contrast to central flipping and rotation, other \glspl{affinetransformation} \cite{Girard.2013}, specifically magnification adjustments \cite{Kuwayama.2019} can change the images shape. One way is to crop (or trim) the image randomly \cite{Gessert.2018}. Another option is to pad the border with either zero values, by repeating the outer most values, or by mirroring the image in the according direction. A special case of this magnification adjustments are scale adjustments, that is, zooming in (center cropping) \cite{BenCohen.2018} or zooming out. Assuming the origin in the middle of the image, zooming is multiplying coordinates by a constant factor with subsequent interpolation. If only one dimension is scaled we refer to it as stretching. Changing the aspect ratio of an image often means information loss and that it has to be interpolated. Kolluru applies cropping only for deeper pixels, because of the limited tissue penetration depth with \acrshort{ivoct} \cite{Kolluru.2018}. Note, that if we combine cropping and padding, we construct a new way of augmenting images, that scales and moves them in the mask. When we decide to crop the same amount as we pad on the opposite side, one often refers to image translation \cite{BenCohen.2018}. Finally, shearing can be used to shift one part of the image to a direction and the other part to the opposite direction \cite{hussain.2018}.
% test different cropping / padding rates
% test if simple zooming in/out is more effective than random cropping / padding
% -> use higher resolution image to avoid loosing information

\subsubsection{Elastic Deformation}

Elastic deformation \cite{Devalla.2018, Simard.2003, Wu.2015} is a warping technique combining shearing and stretching and is often only applied to smaller portions of images. This makes our network invariant to images with atypical morphology. Its goal is to make the network invariant to samples with atypical morphology. Simard showed greater training improvement when training with elastic transformations instead of classical \glspl{affinetransformation} \cite{Simard.2003}. Devalla defined a normalized random displacement field \( u \) representing the random unit displacement vector (values between minus and plus one) for each pixel location in the image as
\begin{equation}
P_{w}=P_{o}+\alpha u
\end{equation}
where \( P_w \) and \( P_o \) are the pixel locations in the warped and the original image. The magnitude of the displacement was controlled by \( \alpha \) and the variation in displacement by \( \sigma \). After the deformation mapping, bilinear (or bicubic and spline) interpolation takes place. \cite{Simard.2003}

\subsubsection{Intensity Shifting}

Intensity shifts constitute an attempt to mimic the natural noise with a specific distribution and intensity for better generalization of the model. Single pixel intensity shifts are among the simplest and most often used \acrshort{da} techniques available. First we present inversion of intensities proposed by Kuwayama \cite{Kuwayama.2019}, which is given by a simple bitwise inversion for unsigned integers. Basic brightness and contrast \cite{Kuwayama.2019} adjustments can be jointly described parallel with the OpenCV documentations \cite{CommunityEditors.2022} in a single formula
\begin{equation}
    f\colon [0,\text{max}] \longrightarrow [0,\text{max}] \text{, } f(x) = a(P_i-h)+h+b
\end{equation}
Here \( p_{ij} \) is the pixel value in column \( i \) and row \( j \) of the image represented by a grid of intensities. A value of \( a > 1 \) increases the contrast (gain), whereas a scalar of \( |a| < 1 \) leads to a decrease. When \( a \) is equal to one, \( f(x) \) equals the identity function such that pixel values remain the same. The \( b \) variable (bias) controls the brightness. Note that \( h \) is defined as \( h = (\text{max}+1)/2 \). Returned values always stay within the boundaries of [0, max], where max is the highest possible value representable, e.g. 255 for an 8 bit unsigned integer type. Brightness raises or lowers the entire range of tones within the image accordingly. When contrast adjustment is raised the image will have a higher percentage of white and dark pixels and eliminate middle tones. Contrast and brightness can also be applied in a jittering way \cite{hussain.2018}, e.g. Hussain uses a small amount of contrast (+/- one to four intensity values)

% gaussian noise
Another augmentation or pre-transformation technique is to apply a pixelwise noise to the image. There are different variants of noise that can be applied. Gaussian noise \cite{Kuwayama.2019, hussain.2018} is a term from signal processing theory. The signal noise has a probability density function equal to that of a Gaussian distribution. In other words, the values are Gaussian distributed. Hussain uses a variance between 0.1 and 0.9 for this. Additive noise values can also be generated randomly \cite{Kuwayama.2019} in the range of a certain intensity limit. It may only be applied to a certain proportion of pixels, e.g. 5\%. Devella and Kermani make extensive use of additive white noise, where intensities are exclusively increased but not decreased. \cite{Devalla.2018, Kermani.2016}
% salt and pepper noise
In image processing salt-and-pepper noise is referred to as randomly changing a chosen proportion of pixels to white or black (its complete maximum or minimum). In some cases noise is added uniformly (speckle noise), but other options are to only add white pixels, or only add black pixels (salt or pepper).
% Mult Noise
In signal processing often multiplicative noise (a.k.a. multiplicative speckle noise) \cite{Devalla.2018, Girard.2013, Kermani.2016} is signal dependent and difficult to remove. \cite{Chen.2012} Thus one can add multiplicative noise manually in an attempt to make the \acrshort{dl} model learn to deal with such inconsistencies. Due to the coherent nature of image acquisition processes, the standard additive noise, so prevalent in image processing, is inadequate \cite{Chen.2012}. Multiplicative noise is dependent on the image data and an accurate description of coherent imaging systems is provided by multiplicative noise models, in which the noise field is multiplied with the original image \cite{BioucasDiasJoseM..2010}. Note that many filtering methods work well when the multiplicative noise is weak \cite{Chen.2012}. Therefore artificial speckle noise is utilized to make the \acrshort{dl} models invariant to its presence.

Non-linear intensity shifts constitute a more complex and expensive technique \cite{Lang.2018, Devalla.2018}.
% Davella
According to Devalla \cite{Devalla.2018}, these can make \acrshort{nn}s invariant to intensity inhomogeneity within and between tissue layers in \acrshort{oct} imaging as follows:
\begin{equation}
\bar{I}_a =-a+(a+b+1) \times I^{c}.
\end{equation}
\( a \) and \( b \) are random numbers between 0 and 0.1, whereas the exponential factor \( c \) lies randomly between 0.6 and 1.4. The transformation is applied to the image intensities \( I \) pixelwise for the i'th column and j'th row, returning the modified image \( \bar{I}_a \).

\begin{comment}
Another intensity shifting technique has been introduced by Hussain, where all pixels of the image \( I \) are elementwise taken to the power \( p \). The augmented image \( I_a \) is then returned by fuction defined as
\begin{equation}
    I_a = \text{sign}(I) \times |I|^{n \times r + 1}
\end{equation}
The power \( p \) is calculated using a random float \( n \) taken from a Gaussian distribution with mean 0 and variance 1. \( r \) is a number less than 1.
\end{comment}

\subsubsection{Convolutional Filters}

In contrast to adding artificial noise, one can also use convolutional filters to remove noise and enhance images. Often unwanted details can be removed this way. The simplest convolutional filter is the mean filter, that means, every value in the filter map is equal to one. It take the average over all pixels in holds. The number of averaged pixels depends on the kernel size. Compared to the mean filter that distorts the image relatively much, the Gaussian filter \cite{Kolluru.2018} is closer to the original image. In a \acrshort{2d} Gaussian kernel the pixel values correspond to the Gaussian curve and add up to one \cref{fig:gaussianfilter}. The standard deviation can additionally be chosen by a variable, e.g. Kulluru uses a kernel size of 7x7 value of one.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{./figures/gaussian_kernel.png}
    \caption[Gaussian convolution]{An 3x3, 5x5 and 7x7 kernel for Gaussian convolution. \cite{Shipitko.2018}}
    \label{fig:gaussianfilter}
\end{figure}
Third, the median \cite{Kihara.2019} filter is particularly suitable for removing salt and pepper noise. Note that it is not described by convolution, but the values in the neighborhood covered by the mask are sorted and the median, (middle value) is returned as value. Compared to the Gaussian filter (or Gaussian blur), the median filter gives better results in less time \cite{KumarArvind.2020}.

\subsubsection{Partial Masking}

Another option to prevent \gls{overfitting} is partial masking, where portions of the images are randomly erased. These cutouts may vary in size and shape. They are commonly used in form of simple squares with a random size, value and ratio between 1:1 and 1:3. Alternatively parts of the images can be replaced or swapped. An advanced level of masking is occlusion patching, which has been used by Devalla in the domain of \acrshort{ivoct} imaging as well \cite{Devalla.2018}. The occluding patches reduced visibility of certain tissues, making our network invariant to blood vessel shadows. Davella used twenty occluding patches at random positions with a size of 60×20 pixels. The intensity of the entire occluded region was reduced by a factor between 0.2 and 0.8.