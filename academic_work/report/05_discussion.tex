
\chapter{Discussion}

The main objective of this bachelorâ€™s thesis was to develop a pipeline that allows qualitative prediction performance measurement of trained neural networks. The second objective was to investigate the effect of partially new \acrshort{da} techniques when training models with \acrshort{ivoct} data sets.

\section{Proof of Concept}

There were three requirements that the pipeline had to meet in order to fulfill the main objective of this work. First, a model designed for the specific purpose had to be assembled, which means meeting the current state of the art requirements for medical image recognition and finding appropriate hyperparameters to make comparable assumptions for later knowledge transfer. Next, established as well as new pre-processing and augmentation techniques had to be implemented. Third, various state-of-the-art metrics were implemented to monitor and evaluate training processes and test performance. In the following the fulfillment of these requirements is discussed based on the conducted experiments on our pipeline with \acrshort{ivoct} sample data.

\subsection{Model Design}

During the experiments it had to be determined that accuracy is only a very limited measure to select the best model for the performance tests with the help of validation, when training on small \acrshort{ivoct} data sets. A major indicator of this was the test results themselves. To keep track of potential inconsistencies, we tested both the most recently trained model and the model deemed best by the early stopping procedure for each test. Depending on the hyperparameters, in particular the maximum number of epochs, the early stop rate, and the initial learning rate, we observed that the testing performance measures vary greatly between the best and the last training epochs. Furthermore, extremely divergent test results arise for each cross-validation fold. One reason for this could be the way Accuracy itself is computed, which does not take into account the imbalance of the data set and extreme bias of its subsets. In addition to a potentially unfortunate data split, the divergence and inhomogeneity of the data set is considered another reason. As a result, the choice of the best epoch depends immensely on the nature of the validation set. For this reason simply halving the initial learning rate, which reduces the fluctuation and thus enables a more precise choice of the tested model, does not address this effect appropriately. K-fold cross-validation with its associated mean computations could additionally amplify that effect. Since the test set can differ strongly from the validation as well as from the training set, extremely different test results may arise for each fold which makes comparison of single folds meaningless. For example, a high \acrshort{acc} of about 74\% is sometimes achieved together with a \acrshort{sens} of over 90\%, while the \acrshort{spec} is around 50\%. Put simply, if a network is trained with 60 related images and 40 other unrelated images, a model trained on a joint set would likely classify most of the images correctly. In contrast, if two different models are each trained on a subset, they would likely classify the images in a ratio of 60 correct and 40 incorrect images. Nevertheless, due to the strong bias, \acrshort{cv} was essential for our experiments, since the test results of the individual folds partially differed substantially from each other.

Replacing \acrshort{bacc} with the \acrshort{mcc} as a measure for comparing individual folds resulted in a more uniform and precise selection of validated epochs, but did not contribute sufficiently to overcome the problem of high diversity in the data set. In our particular early stopping algorithm an additional criterion was used to trigger the mechanisms. Each epoch is tested not only on the validation set, but also on equally distributed parts of the training set. If the performance of the model applied to the validation set increasingly exceeds that of the test set, this is an indicator of \gls{overfitting} and the early termination mechanism was triggered. For relatively homogeneous data, this method works very well. During the development, however, it had to be determined that the method is not suitable for small, highly diversified \acrshort{ivoct} datasets. Due to the bias, the model sometimes predicts significantly better on either test and validation data in all almost cases. Thus the early stopping criterion may be permanently fulfilled, regardless of how the models performance behaves compared on the two sets. Therefore, we omitted this additional criterion. Still, the choice of epochs to test still remained too noisy. Hence early stopping was omitted and a maximum number of 30 training epochs was set equally for each fold after comparing their individual validation performance. We set an initial learning rate of 5e-7 to reduce fluctuation in some specific folds during training. Due to this choice, the model tends to exhibit symptoms of \gls{overfitting} during training. However, this is relativized to some degree by the use of data augmentation. Since optimisation of performance itself is not objective of this work, we tolerate the over- or underfitting that might occur for each epoch. It is noted that this can have a major influence on the overall performance evaluation. Nevertheless, the methodology is sufficient to clearly identify and investigate the general impact of \acrshort{da} techniques on \acrshort{ivoct} data.

\subsection{Augmentation Impact}

Classical data augmentation techniques have, according to the results, a greater positive effect on the performance of the corresponding generated models than \acrshort{ivoct} specific techniques that only modify smaller sections of the image. Considering that with small \acrshort{ivoct} data sets there is only a very limited possibility for fine tuning, it appears to be a logical conclusion that positional information about desired features, especially with very diversified data, matter more than their nature and robustness to potentially interfering artifacts. This is consistent with the observations that less distinct images with inherently lower cross entropy containing less new information for the network, result in little improvement in performance especially at earlier stages of training. Thus, such techniques are preferred for the optimization of networks trained on less diverse data sets.

\subsubsection{Underlying Model}

As the amount of data increases, it is assumed that \gls{overfitting} occurs later during training. This is caused by the fact that the error can only be reduced slower when diversifying transformations are applied and the learning rate is adjusted correspondingly. The effect of an decreased deviation of the validation performance when testing on the same data set with and without \acrshort{da} supports this statement (see \cref{fig:comp_with_out_da} and \cref{fig:cv_steps}). This may lead to a horizontal shift of the curve center in the gridsearch graph. In order to ensure a certain tolerance of the model, the midpoint therefore seemed to be particularly suitable for initial tests. In order to ensure a certain tolerance against changes in the data set, the center point for tests seemed to be particularly suitable. Inspecting the curves of training loss plotted against the overall batch number while training, supports our choice (see \cref{fig:lr_bs}.

For the smallest of our tested learning rates an early stopping accuracy of 1e-7 suffices, since we observe mostly larger deviations after reaching the maximum of their validation measures. We choose this value in assumption that future training processes do not converge with a smaller rate to their validation accuracy maximum.

Comparing the presented values of the listed initialization and pre-training variants in \cref{tab:init_pretrain}, reveals that the performances of the models vary considerably. Significantly different performance values between \acrshortpl{cv} in a single training indicate the missing ability to generalize features. For this reason, no meaningful tendencies or rules can be derived, when training non-augmented data, even though \acrshort{cv} was applied. Transfer learning is used in all further experiments, since in general no worse values can be expected from pre-training than from random initialization of the weights and biases. Rescaling on the other hand ensures a uniform floating point representation.

Without \acrshort{cv} no meaningful comparison would be applicable, because the individual folds (see \cref{fig:cv_steps}) deviate to much when considered individually. The performance depends fundamentally on each validation or test set and the epoch from which the model is chosen. This effect is particularly noticeable, when sensitivity and specificity are considered individually.

A problem when finding the right hyperparameters for scaling, transfer learning and padding the grayscale image is, that validation performance fluctuates excessively, when no \acrshort{da} is used on small data sets. Therefore it may give less representative results for fine tuning. One way to counteract this would be fine tuning the hyperparameters after the data have been augmented and simultaneously adapt the initial \gls{learning-rate}. Alternatively one could adapt the initial \gls{learning-rate} for every \acrshort{cv} step individually.

\subsubsection{Affine Transformation}

Due to the nature of \acrshort{ivoct}, the model benefits from further generalization regarding the horizontal position of the features. Intuitively spoken the horizontal position does not seem to be of particular relevance for feature classification. Thus rotation or horizontal shifts can reduce the dependence on certain positions of the features. This is also reflected in our results since both transformations have a positive impact on the models performance. Larger vertical shifts additionally require the network to filter out the otherwise rather statically positioned features like catheter reflections, which increase the complexity of learning from small data sets. In most cases vertically shifted images cannot replace realistic images. It was therefore expected that shifts tied to the authentic positions of features are more effective for training with small \acrshort{ivoct} data sets. Our expectation that vertical shifts would therefore result in a smaller overall increase in performance than horizontal shifts was not fulfilled. This indicates that the model can easily filter out certain artifacts, such as catheter reflections, than it can learn the desired feature properties. The bias of the testing set may even reduce the \acrshort{mcc} score slightly. We conclude that the value of repositioning the desired features is higher than the different positioning of the artifacts. However, unproportionally boost could not be identified when training on smaller shifts. For this reason, Davella's proposal to give more weight to smaller shifts \cite{Devalla.2018} is rejected when training on \acrshort{ivoct} data. Since we could not show any measurable improvement, when training on \acrshort{ivoct} data with both, fully horizontal and vertical shifts, we make use of fully horizontal shifting and only little vertical shifting of up to 20 pixels for our strategy only.

It is not possible to draw conclusions about the development of individual test results from an increase in overall performance through stretching, shearing or scaling, as these show both positive and negative trends when compared with the \acrshort{mcc} and \acrshort{bacc} scores. In principle, it can be deduced from the observations of the tests with stretching that it has a positive effect on performance. Shearing on the other hand shows a higher \acrshort{mcc} score only when training with polar representations, which could be related to the fact that additional artifacts are generated in this format. Combined with the fact that shearing also removes information from middle rows, this limits the learning effect for the model.

\subsubsection{Elastic Deformation}

The fact that the performance improvement due to augmentation with elastic deformation is not as large as when the models are trained on cartesian representation may have several reasons. In polar representation all sections are distorted uniformly according to a Gaussian filter. The cartesian transformation is not taken into account, which is why the deformations decrease towards the center of the image. They are amplified with increasing distance to the center. In the case that the elastic deformation is applied after the cartesian transformation, the edges of the circle are deformed as well. This represents a large discrepancy to the realistic images and can also have a negative influence.

\subsubsection{Intensity Shifting}

Random brightness changes of the image are similarly caused by the pre-processing transformations. Small, particularly bright maxima are moderated by bilinear interpolation, since these often affect very few pixels that were averaged with others. This is most likely the reason why an additional random but small brightness changes have only a relatively limited effect. It should be mentioned that interpolation alone is not responsible for the brightness change of the entire image. This is only due to the fact that interpolation reduces maximum values and the subsequent rescaling therefore illuminates the entire image. When examining the histogram, it can also be seen that the brightness of the images is relatively evenly distributed. The added value of augmentation by brightness jittering is therefore very limited anyway.

\subsubsection{Noise Filters}

The observation that data augmentation by adding noise does not result in a detectable increase in performance is in line with our expectations. The model can derive the most important information in \acrshort{ivoct} images using deeper structures. For example, to distinguish the arterial wall or the lumen, it is usually sufficient to measure a regional contrast ratio. For this reason, fine structures seem to play only a minor role in classification tasks of \acrshort{ivoct} images. Various noise filters protect especially the first convolutional layers against \gls{overfitting}. This limits the effect that artificial noise can have on \acrshort{ivoct} recordings. Each time noise is applied, new images are created from which the model could learn. However, the model benefits from this only to a limited extent. In this context, it is important to note that the images come from different pullbacks that are related in content. Two images taken one after the other show only few differences and usually have high \gls{mutualinformation}. However, they have different noise sources, which means that the model already learns the natural noise distribution of the images. Additional noise that has a Gaussian distribution or changes only individual pixels, such as salt and pepper noise, brings a barely measurable advantage. At particularly high intensities, the augmented images deviate too much from the originals, so that classification becomes less effective.

\subsubsection{Partial Masking}

Partial masking mainly contributes to better generalizability. Since it does not improve performance, we conclude that the model already prevents \gls{overfitting} well by methods like pruning or L2 generalization with similar methods well enough. The extra cost of partial masking and guide wire removal can thus be omitted when training with an advanced pipeline. The latter approach could be even more effective. The width of the guidewire artifacts are slightly different. Possibly little but valuable information is lost due to the partial masking.

\subsubsection{Artefact Imitation}

Artificial \acrshort{da} techniques like random distortion, curve variation, column scaling and blood speckle appear almost like realistic images. Only the creation of random guidewires clearly distinguishes an image with features from the real thing. Augmenting the data with our artificial artifacts decreases the performance of the resulting model. We find that the model loses performance when artificial transformations are applied to the original image. This can be due to several reasons. First, there is an imbalance of shown anatomies in the data and their nature allows artificial artifacts to be inserted only in certain locations. The model learns prominently about their position and presence in combination with the associated labels. It does not learn the ability to examine the images for the targeted features, which leads to frequent missclassification and high bias. This can be observed in curve variation and random guide wire augmentation. Column scaling, blood speckle and random distortions are always randomly arranged in the image. This is also reflected in the results. They have only a slight negative influence on the test results. The model learns to deal with the additionally inserted artifacts, but does not benefit from them.

We conclude that, in general, transformations for which we cannot detect significant differences from real images when assessed individually by eye are preferable for augmentation. Also the tested artefacts inserted independently of the images content do not fit well enough such that it helps the model to generalize better. Because of the high complexity to artificially recreate artefacts it is recommended that any augmentation strategies are restricted to simpler techniques. They concentrate on commonly used transformations, e.g. affine transformations and elastic deformations. In particular, the training process does not benefit from artificial artefact insertion.

\subsection{Performance Assessment}

In medical diagnostics, \acrshort{sens} is the ability of a test to correctly identify individuals with the disease, while specificity is the ability of the test to correctly identify individuals without the disease. If the \acrshort{sens} is high, any person who has the disease is likely to be classified as positive. If, on the other hand, the \acrshort{spec} is high, any person who does not have the disease is likely to be classified as negative. We choose precision when we want to have greater confidence in true positive diagnoses and rather diagnose some patients false positive than causing some false negative diagnoses missing out on some injured patients.

In our tests, we assign the same importance to the detection of diseases as to ruling them out. In medical diagnostics a different weighing may be desired. For instance, a false-positive sample classification is often tolerable to some degree, unlike an undetected, misdiagnosed stenosis that may cause greater harm to the patient. For this, a greater weight could be assigned to recall. If, on the other hand, the priority is to avoid false positives, specificity is to be weighted higher. Weighting in the model can be achieved by using weighted metrics or intentionally unbalanced data sets while training.

Even though by \acrshort{bacc} and \acrshort{mcc} the performance discrepancies of the models could be evaluated meaningfully, it must be noted that our models shows difficulties in detecting true negatives. This could also be caused by the composition of the test set and the diversity of the data. Increasing preciseness in measurement may be reached, by manually observing and choosing pullback frame series for manually setting up a test set with balanced labels. A second major influence might be the comparatively low depth of our model. With additional convolutional layers, the model would be able to learn more complex features and thus less frequent features, which would allow the underrepresented negative samples to be classified with greater certainty.

\section{General Remarks}

We conclude our discussion with some general remarks on the limitations and feasibility of using data augmentation and pre-processing in real-world \acrshort{ivoct} classification applications. Fundamental limitations of Deep Learning concerns the precision of diagnosis by labeling specialists, generally determined by the majority of multiple doctors. Another critical aspect is that the \acrshortpl{cnn} cannot diagnose unusual diseases that were not included in the original database. \cite{Kuwayama.2019} Noisy labeling might be caused by tissues which are difficult to distinguish and thus potentially degrade the models performance \cite{Kolluru.2018}.

Although the overall classification accuracy of \acrshortpl{b-scan} in our experiments reaches approximately 80\%, learning methods applied to \acrshort{ivoct} could lead to clinically useful results. During intervention, a cardiologist is interested in plaque deposits rather than in reviewing individual frames. To apply the appropriate plaque modification strategy and decide on appropriate stent size, the interventional cardiologist must identify large circumferential calcifications that might hamper stent deployment and lipidous lesions \cite{Kolluru.2018}. Thus, high resolution is not required for any of these scenarios. Processing (including pre-processing and classification) time with less than 0.4 seconds suggests that live-time clinical usage is possible. \cite{Kolluru.2018} In this sense, the results strongly encourage further research to improve \acrshort{ivoct} scan classification.
